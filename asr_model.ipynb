{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition with Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "As part of the capstone project for Udacity's Natural Language Processing Nanodegree program, I have built a deep neural network and integrated it with an end-to-end automatic speech recognition (ASR) pipeline. The pipeline gets raw audio as input and returns a predicted transcription of the spoken words in the audio. The full pipeline is summarized in the figure below.\n",
    "\n",
    "<img src=\"images/pipeline.png\">\n",
    "\n",
    "- **STEP 1** is a pre-processing step that converts raw audio to one of two feature representations that are commonly used for ASR-- that is, spectrogram and Mel-Frequency Cepstral Coefficients (MFCCs). \n",
    "- **STEP 2** is an acoustic model, based on my developed deep neural network architecture, which accepts audio features as input and returns a probability distribution over all potential transcriptions.\n",
    "- **STEP 3** in the pipeline takes the output from the acoustic model and returns a predicted transcription.  \n",
    "\n",
    "The notebook is organized as follows:\n",
    "- [The Data](#thedata)\n",
    "- [**STEP 1**](#step1): Acoustic Features for Speech Recognition\n",
    "- [**STEP 2**](#step2): Deep Neural Network for Acoustic Modeling\n",
    "    - [Model](#Model): CNN + Bidirectional RNN + TimeDistributed Dense\n",
    "    - [Compare the Models](#compare)\n",
    "- [**STEP 3**](#step3): Obtain Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import Audio\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# allocate 50% of GPU memory\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "# import functions for visualizing the extracted features from raw audio\n",
    "from data_generator import vis_train_features, plot_raw_audio, plot_spectrogram_feature, plot_mfcc_feature\n",
    "\n",
    "# import deep nueral network architecture for ASR model\n",
    "from model_utils import asr_model\n",
    "\n",
    "# import function for training the ASR model\n",
    "from train_utils import train_model\n",
    "\n",
    "# import function for getting transcription prediction from the ASR model\n",
    "from prediction_utils import get_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parts of this notebook will be added soon!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
